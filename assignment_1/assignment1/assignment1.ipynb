{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import sklearn\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy\n",
    "import random\n",
    "import gzip\n",
    "import math\n",
    "import string\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "from implicit import bpr\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)\n",
    "\n",
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        u,b,r = l.strip().split(',')\n",
    "        r = int(r)\n",
    "        yield u,b,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, y):\n",
    "    TP_ = numpy.logical_and(pred, y)\n",
    "    FP_ = numpy.logical_and(pred, numpy.logical_not(y))\n",
    "    TN_ = numpy.logical_and(numpy.logical_not(pred), numpy.logical_not(y))\n",
    "    FN_ = numpy.logical_and(numpy.logical_not(pred), y)\n",
    "\n",
    "    TP = sum(TP_)\n",
    "    FP = sum(FP_)\n",
    "    TN = sum(TN_)\n",
    "    FN = sum(FN_)\n",
    "\n",
    "    acc = (TP + TN)/len(pred)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_dataset = []\n",
    "booksPerUser_all = defaultdict(set)\n",
    "usersPerBook_all = defaultdict(set)\n",
    "ratingsPerBook_all = defaultdict(list)\n",
    "\n",
    "# sparse interaction matrix\n",
    "\n",
    "for l in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    entire_dataset.append(l)\n",
    "\n",
    "random.shuffle(entire_dataset)          # shuffle data to avoid overfitting\n",
    "\n",
    "train_data = entire_dataset[:190000]\n",
    "\n",
    "userIDs, itemIDs = {}, {}\n",
    "for u, b, r in entire_dataset:\n",
    "    booksPerUser_all[u].add(b)\n",
    "    usersPerBook_all[b].add(u)\n",
    "    ratingsPerBook_all[b].append(r)\n",
    "    if not u in userIDs:\n",
    "        userIDs[u] = len(userIDs)\n",
    "    if not b in itemIDs:\n",
    "        itemIDs[b] = len(itemIDs)\n",
    "\n",
    "nUsers, nItems = len(userIDs), len(itemIDs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Have read?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build validation with 50% have read and 50% unread\n",
    "valid_data = []\n",
    "for u, b, _ in entire_dataset[190000:]:\n",
    "    valid_data.append((u, b, 1))\n",
    "notRead_valid_set = []\n",
    "set_of_books = set([b for b in itemIDs])\n",
    "\n",
    "booksPerUser_valid = defaultdict(set)\n",
    "for u, b, r in valid_data:\n",
    "    booksPerUser_valid[u].add(b)\n",
    "\n",
    "for d in valid_data:\n",
    "    # get the books that user have not read\n",
    "    diff = set_of_books.difference(booksPerUser_valid[d[0]])\n",
    "    notRead_valid_set.append(\n",
    "        (d[0], list(diff)[random.randint(0, len(diff)-1)]))\n",
    "    # notRead_valid_set[d[0]].append(list(diff)[random.randint(0, len(diff)-1)]) # get random book for user\n",
    "\n",
    "# adding to current validation pairs of (u,b) of books that have not been read by user\n",
    "valid_data_q1 = valid_data\n",
    "for u, b in notRead_valid_set:\n",
    "    valid_data_q1.append((u, b, 0))\n",
    "\n",
    "random.shuffle(valid_data_q1)\n",
    "\n",
    "items = list(itemIDs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize prediction data structure and test dataset\n",
    "test_dataset = []\n",
    "predictions = open(\"predictions_Read.csv\", 'w')\n",
    "with open(\"pairs_Read.csv\") as test_data:\n",
    "    for l in test_data:\n",
    "        if l.startswith(\"userID\"):\n",
    "            predictions.write(l)\n",
    "            continue\n",
    "        u, b = l.strip().split(',')\n",
    "        test_dataset.append((u, b))\n",
    "        # Check if user and books is in indexing data structure\n",
    "        if u not in userIDs:\n",
    "            userIDs[u] = len(userIDs)\n",
    "        if b not in itemIDs:\n",
    "            itemIDs[b] = len(itemIDs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRbatch(tf.keras.Model):\n",
    "    def __init__(self, K, lamb):\n",
    "        super(BPRbatch, self).__init__()\n",
    "        # Initialize variables\n",
    "        self.betaI = tf.Variable(\n",
    "            tf.random.normal([len(itemIDs)], stddev=0.001))\n",
    "        self.gammaU = tf.Variable(tf.random.normal(\n",
    "            [len(userIDs), K], stddev=0.001))\n",
    "        self.gammaI = tf.Variable(tf.random.normal(\n",
    "            [len(itemIDs), K], stddev=0.001))\n",
    "        # Regularization coefficient\n",
    "        self.lamb = lamb\n",
    "\n",
    "    # Prediction for a single instance\n",
    "    def predict(self, u, i):\n",
    "        p = self.betaI[i] + tf.tensordot(self.gammaU[u], self.gammaI[i], 1)\n",
    "        return p\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (tf.nn.l2_loss(self.betaI) +\n",
    "                            tf.nn.l2_loss(self.gammaU) +\n",
    "                            tf.nn.l2_loss(self.gammaI))\n",
    "\n",
    "    def score(self, sampleU, sampleI):\n",
    "        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)\n",
    "        x_ui = beta_i + tf.reduce_sum(tf.multiply(gamma_u, gamma_i), 1)\n",
    "        return x_ui\n",
    "\n",
    "    def call(self, sampleU, sampleI, sampleJ):\n",
    "        x_ui = self.score(sampleU, sampleI)\n",
    "        x_uj = self.score(sampleU, sampleJ)\n",
    "        return -tf.reduce_mean(tf.math.log(tf.math.sigmoid(x_ui - x_uj)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model on new \"userIDs\" and \"itemIDs\" length because there were some users that DNE in the \"train_interactions.csv.gz\"\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)\n",
    "modelBPR = BPRbatch(5, 0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingStepBPR(model, interactions):\n",
    "    Nsamples = 50000\n",
    "    with tf.GradientTape() as tape:\n",
    "        sampleU, sampleI, sampleJ = [], [], []\n",
    "        for _ in range(Nsamples):\n",
    "            u, i, _ = random.choice(interactions)  # positive sample\n",
    "            j = random.choice(items)  # negative sample\n",
    "            while j in booksPerUser_all[u]:\n",
    "                j = random.choice(items)\n",
    "            sampleU.append(userIDs[u])\n",
    "            sampleI.append(itemIDs[i])\n",
    "            sampleJ.append(itemIDs[j])\n",
    "\n",
    "        loss = model(sampleU, sampleI, sampleJ)\n",
    "        loss += model.reg()\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients((grad, var) for\n",
    "                              (grad, var) in zip(\n",
    "                                  gradients, model.trainable_variables)\n",
    "                              if grad is not None)\n",
    "    return loss.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, objective = 0.4693931\n",
      "iteration 20, objective = 0.43645096\n",
      "iteration 30, objective = 0.44130868\n",
      "iteration 40, objective = 0.4323181\n",
      "iteration 50, objective = 0.42410135\n",
      "iteration 60, objective = 0.42429882\n",
      "iteration 70, objective = 0.42425352\n",
      "iteration 80, objective = 0.4206741\n",
      "iteration 90, objective = 0.4186772\n",
      "iteration 100, objective = 0.41640985\n",
      "iteration 110, objective = 0.41679323\n",
      "iteration 120, objective = 0.41774333\n",
      "iteration 130, objective = 0.41801995\n",
      "iteration 140, objective = 0.41503876\n",
      "iteration 150, objective = 0.41429076\n",
      "iteration 160, objective = 0.4145111\n",
      "iteration 170, objective = 0.41655073\n",
      "iteration 180, objective = 0.41342944\n",
      "iteration 190, objective = 0.4129957\n",
      "iteration 200, objective = 0.4132862\n",
      "iteration 210, objective = 0.41189373\n",
      "iteration 220, objective = 0.41488746\n",
      "iteration 230, objective = 0.4150156\n",
      "iteration 240, objective = 0.4158305\n",
      "iteration 250, objective = 0.414916\n",
      "iteration 260, objective = 0.41441554\n",
      "iteration 270, objective = 0.41581252\n",
      "iteration 280, objective = 0.4124548\n",
      "iteration 290, objective = 0.41334566\n",
      "iteration 300, objective = 0.41272676\n"
     ]
    }
   ],
   "source": [
    "for i in range(300):\n",
    "    obj = trainingStepBPR(modelBPR, entire_dataset)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsScorePerUser_test = defaultdict(list)\n",
    "# Add prediction to prediction data structure\n",
    "for u, b in test_dataset:\n",
    "    pred = modelBPR.predict(userIDs[u], itemIDs[b]).numpy()\n",
    "    itemsScorePerUser_test[u].append((pred, b))\n",
    "\n",
    "# Sort prediction data structure by score\n",
    "for u in itemsScorePerUser_test.keys():\n",
    "    itemsScorePerUser_test[u].sort(reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u: u37758667, items: [(1.0866337, 'b99713185'), (-0.066176794, 'b05213070')]\n",
      "u: u85626045, items: [(3.8862379, 'b29802159'), (0.84620416, 'b00524816'), (0.7505729, 'b31024771'), (-0.8422726, 'b60770713')]\n",
      "u: u70770448, items: [(1.6494749, 'b07327816'), (0.9818396, 'b77746740'), (0.8690423, 'b21349423'), (0.6113306, 'b92959743')]\n",
      "u: u64714864, items: [(2.0634742, 'b52217488'), (0.6568857, 'b32857815'), (-0.20397699, 'b06618138'), (-1.0911531, 'b48279541')]\n",
      "u: u78647159, items: [(1.0642279, 'b63721105'), (0.024226338, 'b00654647'), (-0.33912992, 'b05439735'), (-0.74795514, 'b68809234')]\n",
      "u: u43398119, items: [(0.9899411, 'b51839247'), (0.93687814, 'b08135061')]\n",
      "u: u93156409, items: [(1.2602394, 'b24274613'), (-0.6965865, 'b24931555')]\n",
      "u: u85724496, items: [(2.3662302, 'b39903678'), (0.57600373, 'b15357546')]\n",
      "u: u72905804, items: [(1.0721098, 'b78359727'), (-0.3074209, 'b31964420')]\n",
      "u: u61280144, items: [(2.8069437, 'b93997659'), (0.7869042, 'b05871767')]\n"
     ]
    }
   ],
   "source": [
    "# Checking data in prediction data structure\n",
    "for u in list(itemsScorePerUser_test.keys())[:10]:\n",
    "    print(f\"u: {u}, items: {itemsScorePerUser_test[u]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "y_pred_test = []\n",
    "pred_data = []\n",
    "read_cnt_test = 0\n",
    "unread_cnt_test = 0\n",
    "for u, b in test_dataset:\n",
    "    len_before = len(y_pred_test)\n",
    "    fst_half = len(itemsScorePerUser_test[u])//2\n",
    "    if fst_half == 0 and read_cnt_test <= unread_cnt_test:\n",
    "        y_pred_test.append(1)\n",
    "        pred_data.append((u, b, 1))\n",
    "        # predictions.write(u + ',' + b + \",1\\n\")\n",
    "    elif fst_half == 0 and read_cnt_test > unread_cnt_test:\n",
    "        y_pred_test.append(0)\n",
    "        pred_data.append((u, b, 0))\n",
    "        # predictions.write(u + ',' + b + \",0\\n\")\n",
    "    else:\n",
    "        for sb in itemsScorePerUser_test[u][:fst_half]:\n",
    "            if b in sb:\n",
    "                y_pred_test.append(1)\n",
    "                pred_data.append((u, b, 1))\n",
    "                # predictions.write(u + ',' + b + \",1\\n\")\n",
    "                read_cnt_test += 1\n",
    "                break\n",
    "        if len_before == len(y_pred_test):\n",
    "            y_pred_test.append(0)\n",
    "            pred_data.append((u, b, 0))\n",
    "            # predictions.write(u + ',' + b + \",0\\n\")\n",
    "            unread_cnt_test += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "10000\n",
      "('u06592677', 'b27702770', 1)\n"
     ]
    }
   ],
   "source": [
    "# test that there is 50% read and 50% unread predictions\n",
    "print(len(y_pred_test))\n",
    "print(sum(y_pred_test))\n",
    "print(pred_data[19999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"predictions_Read.csv\", 'w') as prediction_file:\n",
    "    prediction_file.write(\"userID,bookID,prediction\\n\")\n",
    "    for u, b, p in pred_data:\n",
    "        prediction_file.write(u + ',' + b + ',' + str(p) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Predict Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': 'u75242413', 'review_id': 'r45843137', 'rating': 4, 'review_text': \"a clever book with a deeply troubling premise and an intriguing protagonist. Thompson's clean, sparse prose style kept each page feeling light even as some rather heavy existential questions dropped upon them. I enjoyed it. \\n and that cover design is boom-pow gorgeous.\", 'n_votes': 1, 'genre': 'mystery_thriller_crime', 'genreID': 3}\n"
     ]
    }
   ],
   "source": [
    "# Gather all data\n",
    "data = []\n",
    "\n",
    "for d in readGz(\"train_Category.json.gz\"):\n",
    "    data.append(d)\n",
    "\n",
    "print(data[0])\n",
    "\n",
    "# Split training and vlaidation data\n",
    "category_all_data = [d for d in data]\n",
    "category_train_data = category_all_data[:90000]\n",
    "category_valid_data = category_all_data[90000:]\n",
    "# stemmer = PorterStemmer()\n",
    "punct = string.punctuation\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "for d in category_all_data:\n",
    "    rev: string = d['review_text']\n",
    "    rev = rev.lower()                           # lowercase\n",
    "    rev = [c for c in rev if not (c in punct)]  # remove punctuation (char)\n",
    "    rev = ''.join(rev)\n",
    "    words = rev.strip().split()\n",
    "    for w in words:\n",
    "        wordCount[w] += 1\n",
    "\n",
    "# sort word by frequency\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "# get dictionary\n",
    "words_dict = [x[1] for x in counts[:10000]]\n",
    "words_dictID = dict(zip(words_dict, range(len(words_dict))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get document frequency using training data set\n",
    "df = defaultdict(int)\n",
    "for d in category_all_data:\n",
    "    rev: string = d['review_text']\n",
    "    rev = rev.lower()                           # lowercase\n",
    "    rev = [c for c in rev if not (c in punct)]  # remove punctuation (char)\n",
    "    rev = ''.join(rev)\n",
    "    words = rev.strip().split()\n",
    "    for w in set(words):\n",
    "        df[w] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get review_text for each review and compute tf vector\n",
    "def feature(data):\n",
    "    tfidf_vector = [0]*len(words_dict)\n",
    "    text = data['review_text']\n",
    "    text = text.lower()                           # lowercase\n",
    "    text = [c for c in text if not (c in punct)]  # remove punctuation (char)\n",
    "    text = ''.join(text)\n",
    "    words = text.strip().split()\n",
    "    # build tfidf vector\n",
    "    for w in words:\n",
    "        if w in words_dict:\n",
    "            tfidf_vector[words_dictID[w]] = (\n",
    "                math.log2(len(category_all_data)/df[w]))\n",
    "    return tfidf_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [feature(d) for d in category_all_data]\n",
    "y = [d['genreID'] for d in category_all_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\traip\\OneDrive\\Desktop\\CSEstuff\\CSE_158\\assignment_1\\assignment1\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.001)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "test_model = linear_model.LogisticRegression(C=0.001) # 0.743 with 0.001\n",
    "test_model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewID_dict = {}\n",
    "for d in readGz(\"test_Category.json.gz\"):\n",
    "    reviewID_dict[d['review_id']] = d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"predictions_Category.csv\", 'w') as predictions:\n",
    "    for l in open(\"pairs_Category.csv\"):\n",
    "        if l.startswith(\"userID\"):\n",
    "            predictions.write(l)\n",
    "            continue\n",
    "        u, r = l.strip().split(',')\n",
    "        x_test = [feature(reviewID_dict[r])]\n",
    "        ypred = test_model.predict(x_test)\n",
    "        predictions.write(u + ',' + r + \",\" + str(ypred[0]) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1274fbf3fb2acb349aaf0edcc2132b005d389f6cbaae11e6f28d92603912a4a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
